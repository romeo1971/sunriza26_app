# ğŸ¯ BitHuman + ElevenLabs Integration - FINALER PLAN

## âœ… WAS IHR BEZAHLT / NUTZT:

- **BitHuman Cloud**: Avatar Video + Lipsync (Cloud Service!)
- **ElevenLabs**: Custom Voice Clone TTS
- **OpenAI Realtime**: LLM + STT
- **LiveKit Cloud**: Room Management
- **Modal.com**: Agent Hosting (oder euer Server)

---

## ğŸ”¥ DIE WAHRHEIT ÃœBER BITHUMAN:

BitHuman ist **KEIN vollstÃ¤ndiger Service**, sondern:

1. âœ… **Avatar Video Rendering** (Cloud)
2. âœ… **Lipsync** (Cloud)
3. âŒ **KEIN TTS** (muss selbst gebracht werden!)
4. âŒ **KEIN Agent Hosting** (muss selbst laufen!)

**IHR BRAUCHT:**
- Einen **laufenden Agent** (Modal/Server) der:
  - OpenAI fÃ¼r LLM/STT nutzt
  - ElevenLabs fÃ¼r TTS nutzt
  - BitHuman Cloud fÃ¼r Video/Lipsync nutzt
  - In LiveKit Room lÃ¤uft

---

## ğŸ“‹ IMPLEMENTIERUNG - SCHRITT FÃœR SCHRITT:

### **1. Modal Secrets erstellen:**

```bash
# LiveKit Credentials
modal secret create livekit-cloud \
  LIVEKIT_URL="wss://your-project.livekit.cloud" \
  LIVEKIT_API_KEY="APIxxx" \
  LIVEKIT_API_SECRET="xxx"

# BitHuman API Secret
modal secret create bithuman-api \
  BITHUMAN_API_SECRET="sk_bh_xxx"

# OpenAI API Key
modal secret create openai-api \
  OPENAI_API_KEY="sk-proj_xxx"

# ElevenLabs API Key
modal secret create elevenlabs-api \
  ELEVENLABS_API_KEY="xxx" \
  ELEVEN_DEFAULT_VOICE_ID="pNInz6obpgDQGcFmaJgB"

# Firebase Admin (fÃ¼r Voice ID Lookup)
modal secret create firebase-admin \
  FIREBASE_CREDENTIALS='{"type":"service_account",...}'
```

### **2. Agent auf Modal deployen:**

```bash
cd /Users/hhsw/Desktop/sunriza/sunriza26
modal deploy modal_bithuman_elevenlabs.py
```

**Ihr bekommt URL:**
```
https://your-workspace--bithuman-elevenlabs-agent-join.modal.run
```

### **3. Flutter anpassen:**

In `lib/screens/avatar_chat_screen.dart` (Zeile 553-563):

**VORHER:**
```dart
final orchUrl = AppConfig.orchestratorUrl
    .replaceFirst('wss://', 'https://')
    .replaceFirst('ws://', 'http://');
final agentUrl = orchUrl.endsWith('/')
    ? '${orchUrl}agent/join'
    : '$orchUrl/agent/join';
```

**NACHHER:**
```dart
// Modal Agent URL
final agentUrl = 'https://your-workspace--bithuman-elevenlabs-agent-join.modal.run';
```

**ODER** in `.env`:
```env
ORCHESTRATOR_URL=https://your-workspace--bithuman-elevenlabs-agent-join.modal.run
```

### **4. Firebase Voice ID sicherstellen:**

Jeder Avatar braucht:
```
avatars/{id}/liveAvatar/agentId = "A91XMB7113"
avatars/{id}/training/voice/elevenVoiceId = "pNInz6obpgDQGcFmaJgB"
```

### **5. ElevenLabs Plugin installieren (falls existiert):**

```bash
pip install livekit-plugins-elevenlabs
```

**Falls NICHT existiert:**
- Agent nutzt OpenAI Voice als Fallback
- **IHR MÃœSST Custom TTS Wrapper bauen!**

---

## ğŸ¬ ABLAUF WENN USER KLICKT:

```
1. Flutter: "GesprÃ¤ch starten" Button
   â†“
2. Flutter â†’ Backend: POST /livekit/token
   â† Token, Room Name
   â†“
3. Flutter â†’ LiveKit: Room Join
   âœ… Connected
   â†“
4. Flutter â†’ Modal: POST /join
   Body: {
     "room": "room-abc123",
     "agent_id": "A91XMB7113"
   }
   â†“
5. Modal Agent:
   â”œâ”€â”€ LÃ¤dt Voice ID aus Firebase
   â”œâ”€â”€ Joined LiveKit Room
   â”œâ”€â”€ Startet BitHuman Avatar (Video)
   â”œâ”€â”€ Startet ElevenLabs TTS (Audio)
   â””â”€â”€ Startet OpenAI (LLM/STT)
   â†“
6. Flutter: Sieht Avatar Video âœ…
   HÃ¶rt ElevenLabs Voice âœ…
```

---

## âš ï¸ KRITISCHE PUNKTE:

### **1. ElevenLabs Plugin fÃ¼r LiveKit Agents:**

**PRÃœFEN:**
```bash
pip search livekit-plugins-elevenlabs
```

**Falls NICHT existiert:**
Ihr mÃ¼sst Custom TTS Wrapper schreiben:

```python
from livekit.agents import tts

class ElevenLabsTTS(tts.TTS):
    def __init__(self, voice_id: str, api_key: str):
        self.voice_id = voice_id
        self.api_key = api_key
    
    async def synthesize(self, text: str) -> bytes:
        # HTTP Request zu ElevenLabs API
        import aiohttp
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"https://api.elevenlabs.io/v1/text-to-speech/{self.voice_id}",
                headers={"xi-api-key": self.api_key},
                json={"text": text, "model_id": "eleven_monolingual_v1"},
            ) as resp:
                return await resp.read()
```

### **2. BitHuman TTS vs. Custom TTS:**

Im Beispiel-Repo gibt es **KEIN** `tts=` Parameter in `AgentSession`!

**Das bedeutet:**
- BitHuman handled TTS automatisch (mit OpenAI Voice)
- **ABER:** Ihr wollt ElevenLabs!

**LÃ–SUNG:**
`AgentSession` **MIT** `tts=` Parameter nutzen:

```python
session = AgentSession(
    llm=openai.realtime.RealtimeModel(...),
    vad=silero.VAD.load(),
    tts=elevenlabs.TTS(voice_id=...) # â† HIER!
)
```

### **3. Agent Kosten:**

Modal Pricing:
- **CPU**: ~$0.0001/s = ~$0.36/hour
- **Memory (4GB)**: ~$0.00005/GB/s = ~$0.18/hour
- **Total**: ~$0.54/hour pro aktive Conversation

**Optimization:**
- `keep_warm=1` fÃ¼r schnellere Starts
- Agent automatisch beenden nach InaktivitÃ¤t

---

## ğŸš€ NÃ„CHSTE SCHRITTE:

1. **Modal Secrets erstellen** (siehe oben)
2. **Agent deployen**: `modal deploy modal_bithuman_elevenlabs.py`
3. **Flutter URL anpassen**: `.env` ORCHESTRATOR_URL setzen
4. **Testen**: 
   ```bash
   # Health Check
   curl https://your-workspace--bithuman-elevenlabs-agent-join.modal.run/health
   
   # Agent starten
   curl -X POST https://your-workspace--bithuman-elevenlabs-agent-join.modal.run/join \
     -H "Content-Type: application/json" \
     -d '{"room":"test-room","agent_id":"A91XMB7113"}'
   ```
5. **Flutter App testen**: GesprÃ¤ch starten!

---

## ğŸ“Š ZUSAMMENFASSUNG:

### **WAS FUNKTIONIERT:**
âœ… LiveKit Room Join  
âœ… Token Generierung  
âœ… BitHuman Cloud Avatar  
âœ… Firebase Voice ID Storage  

### **WAS FEHLT:**
âŒ Agent lÃ¤uft nirgendwo â†’ **LÃ–SUNG: Modal Deployment**  
âŒ ElevenLabs Integration â†’ **LÃ–SUNG: Custom TTS oder Plugin**  
âŒ Voice ID â†’ Agent Connection â†’ **LÃ–SUNG: Firebase Lookup**  

### **FINALE LÃ–SUNG:**
```
Flutter â†’ LiveKit Room
         â†“
       Modal Agent:
       - BitHuman (Video)
       - ElevenLabs (Audio)
       - OpenAI (LLM)
```

---

## ğŸ¯ ERFOLG = WENN:

1. âœ… User klickt "GesprÃ¤ch starten"
2. âœ… Flutter connected zu LiveKit
3. âœ… Modal Agent started automatisch
4. âœ… User sieht BitHuman Avatar Video
5. âœ… User hÃ¶rt ElevenLabs Custom Voice
6. âœ… Conversation funktioniert natÃ¼rlich
7. âœ… Lipsync ist synchron mit Audio

---

**STATUS:** 
- Agent Code: âœ… Geschrieben
- Modal Deployment: â³ Bereit zum Deployen
- Flutter Integration: âœ… Bereits vorhanden (nur URL Ã¤ndern)
- Testing: â³ Nach Deployment

**NEXT:** Modal Secrets erstellen + `modal deploy` ausfÃ¼hren!

