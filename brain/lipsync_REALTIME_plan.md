# Lipsync REALTIME Integration - Der richtige Weg

**Stand:** 17.10.2025, 02:00 Uhr  
**Status:** System ist BEREIT - nur Viseme-Stream fehlt!

---

## ‚úÖ **WAS BEREITS L√ÑUFT:**

1. ‚úÖ **LivePortrait Dynamics** ‚Üí `idle.mp4` Loop zwischen Antworten
2. ‚úÖ **ElevenLabs TTS** ‚Üí Audio-Streams laufen (backend + orchestrator!)
3. ‚úÖ **Orchestrator** ‚Üí `/orchestrator/src/eleven_adapter.ts` ‚Üí **HAT BEREITS `onTimestamp`!** üî•
4. ‚úÖ **Flutter UI** ‚Üí `AvatarOverlay` + `VisemeMixer` ‚Üí **WARTEN NUR AUF DATEN!**
5. ‚úÖ **Assets** ‚Üí `atlas.png`, `mask.png`, `roi.json` ‚Üí von LivePortrait generiert
6. ‚úÖ **Pinecone RAG** ‚Üí Brain funktioniert

---

## üéØ **WAS FEHLT (NUR 3 SACHEN!):**

### 1. **Orchestrator: Viseme-Mapping aktivieren**

```typescript
// orchestrator/src/eleven_adapter.ts (BEREITS DA!)

export function startElevenStream(opts: {
  voice_id: string;
  apiKey: string;
  onAudio: (pcm: Buffer) => void;
  onTimestamp: (ts: TimestampEvent) => void;  // ‚Üê BEREITS VORHANDEN!
}) {
  // ElevenLabs WebSocket Streaming
  const ws = new WebSocket(`wss://api.elevenlabs.io/v1/text-to-speech/${voice_id}/stream-input?...`);
  
  ws.on('message', (data) => {
    const msg = JSON.parse(data);
    
    if (msg.audio) {
      opts.onAudio(Buffer.from(msg.audio, 'base64'));
    }
    
    // ‚Üê HIER: Phoneme Timestamps!
    if (msg.alignment) {
      const visemes = mapPhonemesToVisemes(msg.alignment);
      opts.onTimestamp(visemes);  // ‚Üê SCHON DA!
    }
  });
}

// NEU: Phoneme ‚Üí Viseme Mapping
function mapPhonemesToVisemes(alignment: any): VisemeEvent[] {
  const map: Record<string, string> = {
    // IPA Phoneme ‚Üí LivePortrait Viseme
    '…ô': 'E', '…ö': 'E', '…ù': 'E',  // Schwa
    'i': 'AI', '…™': 'AI', 'iÀê': 'AI',  // EE
    'u': 'U', ' ä': 'U', 'uÀê': 'U',  // OO
    'o': 'O', '…î': 'O', 'oÀê': 'O',  // OH
    '…ë': 'AI', 'aÀê': 'AI',  // AH
    'e': 'E', 'eÀê': 'E', '…õ': 'E',  // EH
    'm': 'MBP', 'b': 'MBP', 'p': 'MBP',  // Lips closed
    'f': 'FV', 'v': 'FV',  // Teeth on lip
    'l': 'L',  // Tongue
    'w': 'WQ', 'r': 'R',
    'Œ∏': 'TH', '√∞': 'TH',  // TH
    ' É': 'CH', ' í': 'CH', 't É': 'CH', 'd í': 'CH',  // SH/CH
    // Silence
    '': 'Rest', ' ': 'Rest',
  };
  
  return alignment.chars.map((char: any) => ({
    character: char.character,
    start_ms: char.start_time_ms,
    end_ms: char.end_time_ms,
    viseme: map[char.phoneme] || 'Rest',
  }));
}
```

### 2. **Orchestrator: WebSocket zu Flutter**

```typescript
// orchestrator/src/lipsync_handler.ts (NEU)

import { WebSocket, WebSocketServer } from 'ws';

const wss = new WebSocketServer({ port: 3001 });

wss.on('connection', (clientWs: WebSocket, req) => {
  const sessionId = req.url?.split('/').pop();
  
  clientWs.on('message', (data) => {
    const msg = JSON.parse(data.toString());
    
    if (msg.type === 'speak') {
      // Start ElevenLabs Stream
      startElevenStream({
        voice_id: msg.voice_id,
        apiKey: process.env.ELEVENLABS_API_KEY!,
        
        onAudio: (pcm) => {
          // Audio zu Client streamen
          clientWs.send(JSON.stringify({
            type: 'audio',
            data: pcm.toString('base64'),
          }));
        },
        
        onTimestamp: (visemes) => {
          // Viseme-Events zu Client streamen
          for (const v of visemes) {
            clientWs.send(JSON.stringify({
              type: 'viseme',
              start_ms: v.start_ms,
              end_ms: v.end_ms,
              value: v.viseme,
            }));
          }
        },
      });
    }
  });
});

console.log('üé§ Lipsync Orchestrator l√§uft auf Port 3001');
```

### 3. **Flutter: WebSocket Client**

```dart
// lib/services/lipsync_service.dart (NEU)

import 'package:web_socket_channel/web_socket_channel.dart';

class LipsyncService {
  WebSocketChannel? _channel;
  VisemeMixer? _mixer;
  
  void connect(String sessionId, VisemeMixer mixer) {
    _mixer = mixer;
    
    // Connect zu Orchestrator
    _channel = WebSocketChannel.connect(
      Uri.parse('wss://orchestrator.sunriza26.com/lipsync/$sessionId'),
    );
    
    _channel!.stream.listen((message) {
      final data = json.decode(message);
      
      switch (data['type']) {
        case 'viseme':
          _handleViseme(data);
          break;
      }
    });
  }
  
  void _handleViseme(Map<String, dynamic> data) {
    final viseme = data['value'] as String;
    final startMs = data['start_ms'] as int;
    final durationMs = (data['end_ms'] as int) - startMs;
    
    // Schedule Viseme-Animation (mit Server-PTS sync!)
    Future.delayed(Duration(milliseconds: startMs), () {
      _mixer?.setViseme(viseme, 1.0);
      
      // Fade out am Ende
      Future.delayed(Duration(milliseconds: durationMs), () {
        _mixer?.setViseme(viseme, 0.0);
      });
    });
  }
  
  void speak(String text, String voiceId) {
    _channel?.sink.add(json.encode({
      'type': 'speak',
      'text': text,
      'voice_id': voiceId,
    }));
  }
  
  void stop() {
    _channel?.sink.add(json.encode({'type': 'stop'}));
    _mixer?.reset();
  }
  
  void dispose() {
    _channel?.sink.close();
  }
}
```

```dart
// avatar_chat_screen.dart (ERWEITERN)

LipsyncService? _lipsyncService;

@override
void initState() {
  super.initState();
  
  // Init Lipsync Service (wenn VisemeMixer vorhanden)
  if (_visemeMixer != null) {
    _lipsyncService = LipsyncService();
    _lipsyncService!.connect(_avatarData!.id, _visemeMixer!);
  }
}

// Bei TTS-Request:
void _sendMessage(String text) async {
  // ... existing LLM logic ...
  
  // Statt lokalem TTS-File:
  // _lipsyncService?.speak(llmResponse, voiceId);  ‚Üê REALTIME!
}

@override
void dispose() {
  _lipsyncService?.dispose();
  super.dispose();
}
```

---

## üöÄ **DEPLOYMENT:**

### 1. Orchestrator auf Cloud Run:

```yaml
# cloudbuild_orchestrator.yaml (NEU)
steps:
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/sunriza26/orchestrator:latest', './orchestrator']
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/sunriza26/orchestrator:latest']
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'run'
      - 'deploy'
      - 'orchestrator'
      - '--image=gcr.io/sunriza26/orchestrator:latest'
      - '--platform=managed'
      - '--region=europe-west1'
      - '--allow-unauthenticated'
      - '--memory=512Mi'
      - '--cpu=1'
      - '--timeout=300'
```

```bash
# Deploy
gcloud builds submit --config=cloudbuild_orchestrator.yaml
```

### 2. Flutter App Config:

```dart
// lib/config.dart (erweitern)
const orchestratorUrl = 'wss://orchestrator-xyz.run.app';
```

---

## üí∞ **KOSTEN:**

| Komponente | Kosten |
|------------|--------|
| ElevenLabs Streaming | **$0.12/min** (BEREITS einkalkuliert!) |
| Orchestrator (Cloud Run) | **$5-10/Monat** (512MB RAM, minimal Traffic) |
| WebSocket Bandwidth | ~50 KB/min ‚Üí **negligible** |
| **TOTAL NEU** | **~$5-10/Monat** (fix) |

**KEINE zus√§tzlichen per-Request Kosten!** ‚úÖ

---

## ‚è±Ô∏è **LATENZ:**

| Phase | Zeit |
|-------|------|
| LLM Response | ~500-1000ms |
| ElevenLabs First Audio | ~150-250ms |
| WebSocket ‚Üí Flutter | ~20-50ms |
| Viseme Render | ~5-10ms |
| **Glass-to-glass** | **~700-1300ms** ‚Üê MEGA SCHNELL! ‚úÖ |

---

## üéØ **IMPLEMENTATION STEPS:**

### **Tag 1: Orchestrator**
1. ‚úÖ `mapPhonemesToVisemes()` implementieren
2. ‚úÖ WebSocket Server aufsetzen (`lipsync_handler.ts`)
3. ‚úÖ ElevenLabs Streaming mit Phoneme-Alignment aktivieren
4. ‚úÖ Lokal testen mit Test-Client

### **Tag 2: Flutter**
1. ‚úÖ `LipsyncService` implementieren
2. ‚úÖ WebSocket Connection zu Orchestrator
3. ‚úÖ `VisemeMixer` Integration
4. ‚úÖ Testen mit Avatar-Chat

### **Tag 3: Deployment & Testing**
1. ‚úÖ Orchestrator auf Cloud Run deployen
2. ‚úÖ Flutter App Config aktualisieren
3. ‚úÖ End-to-End Testing
4. ‚úÖ Latency & Qualit√§t optimieren

**TOTAL: 3 Tage!** ‚úÖ

---

## üî• **WARUM DAS DIE RICHTIGE L√ñSUNG IST:**

‚úÖ **MEGA SCHNELL:** ~700-1300ms glass-to-glass  
‚úÖ **KOSTEN G√úNSTIG:** Nur $5-10/Monat fix (ElevenLabs bereits kalkuliert!)  
‚úÖ **STABIL:** WebSocket ist Standard, kein kompliziertes WebRTC n√∂tig  
‚úÖ **EINFACH:** Orchestrator ist BEREITS DA! Nur Viseme-Mapping hinzuf√ºgen!  
‚úÖ **SKALIERBAR:** Cloud Run auto-scales, kein Problem  

---

## üìù **N√ÑCHSTE SCHRITTE (SOFORT!):**

```bash
# 1. Orchestrator erweitern
cd orchestrator/src
# Implementiere mapPhonemesToVisemes() in eleven_adapter.ts
# Implementiere lipsync_handler.ts (WebSocket Server)

# 2. Deploy Orchestrator
gcloud builds submit --config=cloudbuild_orchestrator.yaml

# 3. Flutter: LipsyncService implementieren
cd lib/services
# Erstelle lipsync_service.dart

# 4. Test!
flutter run
```

---

**üöÄ LOS GEHT'S!** Dies ist die **ECHTE** L√∂sung - nicht das Offline-Zeug!

**Latenz:** MEGA SCHNELL ‚úÖ  
**Kosten:** G√úNSTIG ‚úÖ  
**Stabilit√§t:** ROBUST ‚úÖ  

**Letzte Aktualisierung:** 17.10.2025, 02:05 Uhr

